{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# About\n",
    "This notebook will produce all the final clean half-hourly energy data for step 3.2 of the data processing for Module 1.\n",
    "\n",
    "**This requires output from the previous steps - 1.1A, 1.1B and 2 - so run those notebooks first**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Required user input**\n",
    "\n",
    "Update the cell below once each for the full years of 2019, 2020, 2021, and run the entire notebook for each.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year = 2021 # Update year - this is the year of data you are working on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't change these.\n",
    "\n",
    "# Source data files from steps 1.1 and 2\n",
    "source_directory_gas='Step_1_1_Outputs'\n",
    "source_directory_elec=source_directory_gas\n",
    "source_subdirectory_gas = 'Step_1_1A_Gas_'+str(year)+'_hh'\n",
    "source_subdirectory_elec = 'Step_1_1B_Elec_'+str(year)+'_hh'\n",
    "source_directory_temperature = 'Step_2_Outputs'\n",
    "source_filename_temperature = 'Step_2_Temp_'+str(year)+'_hh.csv'\n",
    "\n",
    "# Index for the year, UTC - note that this must start at 1 Jan, 00:30:00, and end the following 1 Jan, 00:00:00\n",
    "index_start_date=str(year)+'-01-01 00:30:00' # Start date for the output's index to include.\n",
    "index_end_date=str(year+1)+'-01-01 00:00:00' # End date for the output's index to include.\n",
    "\n",
    "output_directory= 'Module_1_final_outputs'\n",
    "output_subdirectory='hh_'+str(year)\n",
    "output_filename_suffix ='_annual_report_sm_hh_'+str(year)+'.csv' # Names will start with PURPN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get list of grid_cells mapped to PUPRN, from the participant data file\n",
    "puprn_to_grid_cell = pd.read_csv(os.path.join(locations.serl_data_path,locations.participant_data_file),\n",
    "                                 usecols=['PUPRN','grid_cell'],\n",
    "                                 index_col='PUPRN')\n",
    "\n",
    "# Load the temperature data\n",
    "temperature_data_hh = pd.read_csv(os.path.join(source_directory_temperature,source_filename_temperature),\n",
    "                                 parse_dates=['date_time_utc'],\n",
    "                                 index_col='date_time_utc')\n",
    "temperature_data_hh.index=temperature_data_hh.index.tz_localize(tz='UTC')\n",
    "temperature_data_hh.index.names=['Read_date_time_UTC']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loop through the list of PUPRNs for both fuels\n",
    "We want to create a list of the final gas data first, then the final electricity data, then merge them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get list of gas PUPRNs to work with, from the source directory\n",
    "puprn_filelist_gas = [f for f in os.listdir(os.path.join(source_directory_gas,source_subdirectory_gas)) if os.path.isfile(os.path.join(source_directory_gas,source_subdirectory_gas, f))]\n",
    "puprn_filelist_gas = sorted(puprn_filelist_gas, key=str.lower)\n",
    "print('Check this is how many gas PUPRN hh files you were expecting to find:\\n',\n",
    "      len(puprn_filelist_gas))\n",
    "\n",
    "#Get list of electricity PUPRNs to work with, from the source directory\n",
    "puprn_filelist_elec = [f for f in os.listdir(os.path.join(source_directory_elec,source_subdirectory_elec)) if os.path.isfile(os.path.join(source_directory_elec,source_subdirectory_elec, f))]\n",
    "puprn_filelist_elec = sorted(puprn_filelist_elec, key=str.lower)\n",
    "print('Check this is how many elec PUPRN hh files you were expecting to find:\\n',\n",
    "      len(puprn_filelist_elec))\n",
    "\n",
    "# Make a superlist of PUPRNs that are in at least one of the lists.\n",
    "# What's in puprn_filelist_gas that's not in puprn_filelist_elec\n",
    "differences = list(set(puprn_filelist_gas).difference(puprn_filelist_elec))\n",
    "puprn_filelist = sorted((puprn_filelist_elec + differences), key=str.lower)\n",
    "print('There are this many unique PUPRNs with at least one fuel of hh data:\\n',len(puprn_filelist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll be processing each PUPRN's data and saving it to separate csvs one PUPRN at a time.\n",
    "puprns_saved=0\n",
    "puprn_errors=[]\n",
    "puprn_temp_nans=[]\n",
    "puprn_out_of_sequence=[]\n",
    "puprn_duplicate_rows=[]\n",
    "# First, create the template index - a complete year. 30 minute time steps.\n",
    "date_time_index_new = pd.date_range(index_start_date,index_end_date,freq='30T')\n",
    "date_time_index_new =date_time_index_new.tz_localize(tz='UTC')\n",
    "date_time_index_new\n",
    "\n",
    "# Create the output folders if they're not already there\n",
    "if not os.path.exists(os.path.join(output_directory,output_subdirectory,'Errors')):\n",
    "    os.makedirs(os.path.join(output_directory,output_subdirectory,'Errors'))\n",
    "\n",
    "for i in puprn_filelist:\n",
    "    # Load data or create blank dataframes instead\n",
    "    try:\n",
    "        temp_data_gas = pd.read_csv(os.path.join(source_directory_gas,source_subdirectory_gas,i),\n",
    "                                    usecols=['PUPRN','Read_date_time_UTC','Gas_hh_Wh'],\n",
    "                                    index_col=['Read_date_time_UTC'],\n",
    "                                    parse_dates=['Read_date_time_UTC'])\n",
    "        temp_data_gas.index=temp_data_gas.index.tz_localize(tz='UTC')\n",
    "    except: \n",
    "        temp_data_gas=pd.DataFrame(index=date_time_index_new,\n",
    "                                   columns=['PUPRN','Gas_hh_Wh'])\n",
    "        temp_data_gas.index=temp_data_gas.index.set_names('Read_date_time_UTC')\n",
    "    \n",
    "    try:\n",
    "        temp_data_elec = pd.read_csv(os.path.join(source_directory_elec,source_subdirectory_elec,i),\n",
    "                                     usecols=['PUPRN','Read_date_time_UTC','Elec_act_net_hh_Wh'],\n",
    "                                     index_col=['Read_date_time_UTC'],\n",
    "                                     parse_dates=['Read_date_time_UTC'])\n",
    "        temp_data_elec.index=temp_data_elec.index.tz_localize(tz='UTC')\n",
    "    except:\n",
    "        temp_data_elec=pd.DataFrame(index=date_time_index_new,\n",
    "                                    columns=['PUPRN','Elec_act_net_hh_Wh'])\n",
    "        temp_data_elec.index=temp_data_elec.index.set_names('Read_date_time_UTC')\n",
    "\n",
    "    # Create a blank df for the data to be joined into\n",
    "    energy_data_final = pd.DataFrame(index=date_time_index_new)\n",
    "    energy_data_final.index=energy_data_final.index.set_names('Read_date_time_UTC')\n",
    "\n",
    "    # Join energy data onto it\n",
    "    energy_data_final = pd.merge(energy_data_final,temp_data_elec,left_index=True,right_index=True, how='outer')\n",
    "    energy_data_final = pd.merge(energy_data_final,temp_data_gas,left_index=True,right_index=True,on='PUPRN', how='outer')\n",
    "    #Rename to the final column names\n",
    "    energy_data_final.rename(columns={'Elec_act_net_hh_Wh':'Clean_elec_net_Wh','Gas_hh_Wh':'Clean_gas_Wh'},inplace=True)\n",
    "    \n",
    "    # Join temperature data onto it\n",
    "    grid_cell = puprn_to_grid_cell.at[i[:-4],'grid_cell']\n",
    "    temp_data_temperatures = temperature_data_hh[temperature_data_hh.grid_cell==grid_cell]\n",
    "    energy_data_final = energy_data_final.join(temp_data_temperatures, how='left')\n",
    "    \n",
    "    # This is now comprised of clean energy data, and ready except for the local time cols, as the index is UTC.\n",
    "    # Recreate the SERL-style Read_date_time_local, and read_date_effective_local\n",
    "    energy_data_final['Read_date_time_local']=energy_data_final.index.tz_convert(tz='Europe/London')\n",
    "    energy_data_final['Read_date_time_local_midpoint']=energy_data_final.Read_date_time_local- pd.Timedelta(minutes=15)\n",
    "    energy_data_final.Read_date_time_local_midpoint=energy_data_final.Read_date_time_local_midpoint.astype('str')\n",
    "    energy_data_final['Read_date_effective_local']=energy_data_final.Read_date_time_local_midpoint.str.split(None).str[0]\n",
    "    energy_data_final.Read_date_time_local=energy_data_final.Read_date_time_local.astype('str')\n",
    "    energy_data_final.Read_date_time_local=energy_data_final.Read_date_time_local.replace({'\\+00:00':' GMT'},regex=True)\n",
    "    energy_data_final.Read_date_time_local=energy_data_final.Read_date_time_local.replace({'\\+01:00':' BST'},regex=True)\n",
    "\n",
    "    # Create Readings_from_midnight_local\n",
    "    energy_data_final['Read_time_local']=energy_data_final.Read_date_time_local.str.split(None).str[1]\n",
    "    energy_data_final['Read_time_local_midpoint']=(pd.to_datetime(energy_data_final.Read_time_local,format=\"%H:%M:%S\") \n",
    "                                  - pd.Timedelta(minutes=15))\n",
    "    energy_data_final['Readings_from_midnight_local'] = (energy_data_final.Read_time_local_midpoint.dt.hour\n",
    "                                          + energy_data_final.Read_time_local_midpoint.dt.minute/60)*2 +0.5\n",
    "\n",
    "    # PUPRN is only filled for rows with energy data. This is fixed below.\n",
    "    energy_data_final.PUPRN = i[:-4]\n",
    "    \n",
    "    #For 2021 only, the last data point of temperature data is missing as it is not available in the 4th Edition Obseratory data release, so we will forward fill from the previous reading.\n",
    "    if year == 2021:\n",
    "        energy_data_final.temp_C.fillna(method='ffill',limit=1,inplace=True)\n",
    "    \n",
    "    # Check for data errors, and save the relevant output (in a subfolder if there's an error)\n",
    "    # - Make a note if the home has something apparently wrong with it.\n",
    "    subfolder=''\n",
    "    no_temp_nans = (energy_data_final.temp_C.isnull().sum()==0)\n",
    "    in_sequence = energy_data_final.index.is_monotonic_increasing\n",
    "    no_duplicate_rows = energy_data_final.index.is_unique\n",
    "    all_rows_as_expected = (in_sequence & no_duplicate_rows & no_temp_nans)\n",
    "    if energy_data_final.shape[0]!=len(date_time_index_new) or all_rows_as_expected==False:\n",
    "        puprn_errors.append(i[:-4])\n",
    "        if no_temp_nans == False:\n",
    "            puprn_temp_nans.append(i[:-4])\n",
    "        if in_sequence == False:\n",
    "            puprn_out_of_sequence.append(i[:-4])\n",
    "        if no_duplicate_rows == False:\n",
    "            puprn_duplicate_rows.append(i[:-4])\n",
    "        subfolder='Errors'\n",
    "    energy_data_final[['PUPRN','Read_date_time_local','Read_date_effective_local','Readings_from_midnight_local','Clean_elec_net_Wh','Clean_gas_Wh','temp_C']].to_csv(os.path.join(output_directory,output_subdirectory,subfolder,i[:-4]+output_filename_suffix), index=True)\n",
    "    puprns_saved=puprns_saved+1\n",
    "\n",
    "    # Note progress occasionally (every 250 homes):\n",
    "    if puprns_saved % 250 == 0:\n",
    "        print(puprns_saved,\"PUPRNs of data have been processed. Continuing...\")\n",
    "        if len(puprn_errors)>0:\n",
    "            print(\"You've got\",len(puprn_errors),\"PUPRNs with errors so far though (saved in the subfolder 'Errors'), of which:\\n\",\n",
    "                 len(puprn_temp_nans),\"have missing temperature readings;\\n\",\n",
    "                 len(puprn_out_of_sequence),\"have out of sequence datetime rows;\\n\",\n",
    "                 len(puprn_duplicate_rows),\"have duplicate rows.\")\n",
    "\n",
    "print('\\nJob done, total PUPRNs gone through =',puprns_saved,\n",
    "      '\\nOf which, this many had errors:',len(puprn_errors),\", of which:\\n\",\n",
    "                 len(puprn_temp_nans),\"have missing temperature readings;\\n\",\n",
    "                 len(puprn_out_of_sequence),\"have out of sequence datetime rows;\\n\",\n",
    "                 len(puprn_duplicate_rows),\"have duplicate rows.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If there are any, save list of PUPRNs with errors (pandas is actually the neatest way to save a list to csv!)\n",
    "if len(puprn_errors)>0:\n",
    "    pd.Series(puprn_errors).to_csv(os.path.join(output_directory,'PUPRNs_with_hh_data_errors_'+str(year)+'.csv'), index=False)\n",
    "    print(\"\\nThe full list of PUPRNs with errors is saved in the same output folder as 'PUPRNs_with_hh_data_errors_'\"+str(year)+\".csv'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
