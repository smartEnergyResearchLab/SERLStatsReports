{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# About\n",
    "This notebook will produce the temperature data for step 2 of the data processing for Module 1.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Required user input**\n",
    "\n",
    "Update the cell below once each for the full years of 2019, 2020, 2021, and run the entire notebook for each.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year = 2021 # Update year - this is the year of data you are working on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't change these.\n",
    "\n",
    "# Index for the year, UTC - note that this must start at 1 Jan, 00:00:00, and end the following 1 Jan, 00:00:00\n",
    "index_start_date=str(year)+'-01-01 00:00:00' # Start date for the output's index to include.\n",
    "index_end_date=str(year+1)+'-01-01 00:00:00' # End date for the output's index to include.\n",
    "\n",
    "output_directory='Step_2_Outputs'\n",
    "output_filename_daily = 'Step_2_Temp_'+str(year)+'_daily.csv'\n",
    "output_filename_hh = 'Step_2_Temp_'+str(year)+'_hh.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import locations\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get list of grid_cells to work with, from the participant data file\n",
    "participant_data = pd.read_csv(os.path.join(locations.serl_data_path,locations.participant_data_file),\n",
    "                               usecols=['PUPRN','grid_cell'])\n",
    "grid_cell_list = sorted(participant_data.grid_cell.unique().tolist(), key=str.lower)\n",
    "print(len(grid_cell_list),'grid cells found.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll be appending each month of data to a single dataframe to save. Create a blank csv with just headers first, otherwise we either get no headers or duplicate headers below.\n",
    "temperature_data_hourly = pd.DataFrame(columns=['grid_cell','date_time_utc','2m_temperature_K','temp_C'])\n",
    "\n",
    "# Create a list from 01 of the year being processed to 01 of the subsequent year (we need one data point for each grid_cell from 1st Jan of the following year)\n",
    "months = [str(item).zfill(2) for item in list((range(1,13)))]\n",
    "year_months = [str(year)+'_'+i for i in months]\n",
    "year_months.append(str(year+1)+'_01') # Comment this out if this file is not available in the data release.\n",
    "days_count = (datetime.strptime(index_end_date, '%Y-%m-%d %H:%M:%S') - datetime.strptime(index_start_date, '%Y-%m-%d %H:%M:%S')).days #+31\n",
    "\n",
    "for i in year_months:\n",
    "    temp_temperature = pd.read_csv(os.path.join(locations.serl_data_path,\n",
    "                                                locations.climate_directory,\n",
    "                                               'serl_climate_data_'+i+'_edition04.csv'),\n",
    "                                   index_col=False,\n",
    "                                   usecols=['grid_cell','date_time_utc','analysis_date','2m_temperature_K'],\n",
    "                                   parse_dates=['date_time_utc'])\n",
    "    # Drop grid_cells we don't need\n",
    "    temp_temperature = temp_temperature[temp_temperature.grid_cell.isin(grid_cell_list)]\n",
    "    temperature_data_hourly = temperature_data_hourly.append(temp_temperature)\n",
    "\n",
    "# Calculate temp_C\n",
    "temperature_data_hourly.temp_C=temperature_data_hourly['2m_temperature_K']-273.15\n",
    "# Set index\n",
    "temperature_data_hourly.set_index(['grid_cell','date_time_utc'],inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare half-hourly data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill in those half-hours\n",
    "date_time_index_new = pd.date_range(index_start_date,index_end_date,freq='30T')\n",
    "temperature_data_hh = pd.DataFrame(index=pd.MultiIndex.from_product([grid_cell_list,date_time_index_new],\n",
    "                                                             names=['grid_cell','date_time_utc']))\n",
    "# Join onto it\n",
    "temperature_data_hh = pd.merge(temperature_data_hh,temperature_data_hourly['temp_C'].to_frame(),\n",
    "                               left_index=True,right_index=True, how='left')\n",
    "# Ffill the gaps\n",
    "temperature_data_hh.fillna(method='ffill',limit=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some basic data quality checks\n",
    "# No Nans; correct length; monotonically increasing.\n",
    "nancount = temperature_data_hh.temp_C.isnull().sum()\n",
    "in_sequence = temperature_data_hh.index.is_monotonic_increasing \n",
    "no_duplicate_rows = temperature_data_hh.index.is_unique\n",
    "wrong_length = temperature_data_hh.shape[0] - len(grid_cell_list)*len(date_time_index_new)\n",
    "if (nancount== 0 and in_sequence==True and no_duplicate_rows== True and wrong_length==0):\n",
    "    print('Data is sorted by grid_cell then datetime, has no duplicates (no duplicate grid_cell and date_time_utc combinations), and has no missing rows of temp_C data.')\n",
    "else:\n",
    "    print(\"WARNING! Your data has one or more issues:\\n- This many missing values (should be zero):\",\n",
    "          nancount,\n",
    "          \"\\n- Index out of sequence:\",\n",
    "          (not in_sequence),\n",
    "          \"\\n- Duplicate rows (duplicate grid_cell and date_time_utc combination):\",\n",
    "          (not no_duplicate_rows),\n",
    "          \"\\n- This many rows too long (or too short, if negative):\",\n",
    "          wrong_length,\n",
    "          \"\\nCheck and fix before continuing.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save hh output\n",
    "if not os.path.exists(os.path.join(output_directory)):\n",
    "    os.makedirs(os.path.join(output_directory))\n",
    "temperature_data_hh['temp_C'].to_csv(os.path.join(output_directory,output_filename_hh),header=True,index=True)\n",
    "print(\"Job done. Everything saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare daily data\n",
    "We'll calculate:\n",
    "* daily mean C\n",
    "* heating degree days, following the Spinoni et al 2015 method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temperature_data_hourly_b=temperature_data_hourly.reset_index()\n",
    "temperature_data_hourly_b['Read_date_effective_local']=temperature_data_hourly_b.date_time_utc.dt.tz_localize(tz='UTC').dt.tz_convert(tz='Europe/London').dt.date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is adapted from the method described in McKenna et al (2022) DOI: 10.1016/j.enbuild.2022.111845..\n",
    "# Create the daily df\n",
    "temperature_data_daily = temperature_data_hourly_b[['grid_cell','Read_date_effective_local',\n",
    "                                        'temp_C']].groupby(by=['grid_cell','Read_date_effective_local']).agg(['mean','max','min'])\n",
    "\n",
    "temperature_data_daily.columns=temperature_data_daily.columns.droplevel(0)\n",
    "temperature_data_daily.rename(columns={'mean':'mean_temp_C','max':'T_X','min':'T_N'},inplace=True)\n",
    "temperature_data_daily['T_M']= (temperature_data_daily.T_X + temperature_data_daily.T_N)/2\n",
    "\n",
    "T_b = 15.5\n",
    "temperature_data_daily['hdd']=0\n",
    "\n",
    "logic = temperature_data_daily.T_X <= T_b\n",
    "temperature_data_daily.loc[logic,'hdd'] = T_b - temperature_data_daily.loc[logic,'T_M']\n",
    "logic = (temperature_data_daily.T_X > T_b) & (temperature_data_daily.T_M <= T_b)\n",
    "temperature_data_daily.loc[logic,'hdd'] = (T_b - temperature_data_daily.loc[logic,'T_N']) / 2 - (temperature_data_daily.loc[logic,'T_X'] - T_b) /4\n",
    "logic = (temperature_data_daily.T_M > T_b) & (temperature_data_daily.T_N <= T_b)\n",
    "temperature_data_daily.loc[logic,'hdd'] = (T_b - temperature_data_daily.loc[logic,'T_N']) / 4\n",
    "\n",
    "# Round outputs to 2d.p.\n",
    "temperature_data_daily = temperature_data_daily.round(decimals=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some basic data quality checks\n",
    "# No Nans; correct length; monotonically increasing; no duplicate rows.\n",
    "nancount = temperature_data_daily[['mean_temp_C','hdd']].isnull().sum().sum()\n",
    "in_sequence = temperature_data_daily.index.is_monotonic_increasing \n",
    "no_duplicate_rows = temperature_data_daily.index.is_unique\n",
    "wrong_length = temperature_data_daily.shape[0] - len(grid_cell_list)*days_count\n",
    "if (nancount== 0 and in_sequence==True and no_duplicate_rows== True and wrong_length==0):\n",
    "    print('Data is sorted by grid_cell then Read_date_effective_local, has no duplicates (no duplicate grid_cell and analysis_date combinations), and has no missing rows of mean_temp_C or hdd data.')\n",
    "else:\n",
    "    print(\"WARNING! Your data has one or more issues:\\n- This many mean_temp_C or hdd missing values (should be zero):\",\n",
    "          nancount,\n",
    "          \"\\n- Index out of sequence:\",\n",
    "          (not in_sequence),\n",
    "          \"\\n- Duplicate rows (duplicate grid_cell and Read_date_effective_local combination):\",\n",
    "          (not no_duplicate_rows),\n",
    "          \"\\n- This many rows too long (or too short, if negative):\",\n",
    "          wrong_length,\n",
    "          \"\\nCheck and fix before continuing.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save daily output\n",
    "if not os.path.exists(os.path.join(output_directory)):\n",
    "    os.makedirs(os.path.join(output_directory))\n",
    "temperature_data_daily[['mean_temp_C','hdd']].to_csv(os.path.join(output_directory,output_filename_daily),header=True,index=True)\n",
    "print(\"Job done. Everything saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
